================================================================================
F1 PREDICTOR - FINE-TUNING GUIDE (IDIOT-PROOF EDITION)
================================================================================

This guide will help you fine-tune Qwen 2.5 3B on Kaggle for FREE.
Total time: ~1 hour
Difficulty: Copy-paste level

================================================================================
PART 1: SETUP KAGGLE (10 minutes)
================================================================================

STEP 1: Create Kaggle Account
-----------------------------
1. Go to: https://www.kaggle.com
2. Click "Register" (top right)
3. Sign up with Google (easiest) or email
4. Verify your email if needed

STEP 2: Phone Verify (REQUIRED for GPU)
---------------------------------------
1. Click your profile icon (top right)
2. Click "Settings"
3. Scroll to "Phone Verification"
4. Enter your phone number
5. Enter the code they text you
   
   ‚ö†Ô∏è WITHOUT THIS, YOU CANNOT USE FREE GPU!

STEP 3: Get Hugging Face Token
------------------------------
1. Go to: https://huggingface.co
2. Create account (or login)
3. Click profile icon ‚Üí "Settings"
4. Click "Access Tokens" (left sidebar)
5. Click "New token"
6. Name: "kaggle" 
7. Type: "Write"
8. Click "Generate"
9. COPY THE TOKEN (you won't see it again!)

   ‚úÖ That's it! Qwen 2.5 3B is ungated - NO license approval needed!


================================================================================
PART 2: UPLOAD YOUR DATA (5 minutes)
================================================================================

STEP 1: Create Dataset on Kaggle
--------------------------------
1. Go to: https://www.kaggle.com/datasets
2. Click "+ New Dataset"
3. Name it: "f1-constructor-data"
4. Keep it "Private"

STEP 2: Upload Files
--------------------
Upload these 3 files from your "F1 bot/data" folder:
   - constructor_standings.csv
   - constructors.csv  
   - races.csv

STEP 3: Click "Create"
----------------------
Wait for upload to complete.


================================================================================
PART 3: CREATE NOTEBOOK (5 minutes)
================================================================================

STEP 1: Create New Notebook
---------------------------
1. Go to: https://www.kaggle.com/code
2. Click "+ New Notebook"

STEP 2: Enable GPU
------------------
1. Click "..." (three dots) on right side
2. Click "Accelerator"
3. Select "GPU P100" (or T4 if P100 unavailable)
4. Click "Save"

   ‚ö†Ô∏è YOU MUST DO THIS OR TRAINING WILL TAKE FOREVER!

STEP 3: Add Your Dataset
------------------------
1. Click "+ Add data" (right sidebar)
2. Search "f1-constructor-data" (your dataset)
3. Click "Add"

STEP 4: Add Hugging Face Secret
-------------------------------
1. Click "Add-ons" ‚Üí "Secrets"
2. Click "Add a new secret"
3. Label: HF_TOKEN
4. Value: (paste your Hugging Face token from Part 1)
5. Click "Save"


================================================================================
PART 4: COPY-PASTE THE CODE (2 minutes)
================================================================================

Delete everything in the notebook and paste this ENTIRE code block:

--------------------------------------------------------------------------------
# CELL 1: Install packages (run this first, wait 2-3 min)
--------------------------------------------------------------------------------

!pip install -q transformers datasets peft accelerate bitsandbytes trl

--------------------------------------------------------------------------------
# CELL 2: Setup and Login (run after Cell 1 finishes)
--------------------------------------------------------------------------------

import os
from kaggle_secrets import UserSecretsClient
secrets = UserSecretsClient()
os.environ["HF_TOKEN"] = secrets.get_secret("HF_TOKEN")

from huggingface_hub import login
login(token=os.environ["HF_TOKEN"])

print("‚úÖ Logged in to Hugging Face!")

--------------------------------------------------------------------------------
# CELL 3: Load and Prepare Data (run after Cell 2)
--------------------------------------------------------------------------------

import pandas as pd

# Load your data
standings = pd.read_csv("/kaggle/input/f1-constructor-data/constructor_standings.csv")
races = pd.read_csv("/kaggle/input/f1-constructor-data/races.csv")
constructors = pd.read_csv("/kaggle/input/f1-constructor-data/constructors.csv")

# Merge data
merged = pd.merge(standings, races[['raceId', 'year', 'round']], on='raceId')
merged = pd.merge(merged, constructors[['constructorId', 'name']], on='constructorId')

# Get final standings per season
seasonal = merged.loc[merged.groupby(['year', 'constructorId'])['round'].idxmax()]
seasonal = seasonal.sort_values(['year', 'points'], ascending=[True, False])

# Create training examples
training_data = []

for year in seasonal['year'].unique():
    if year < 2010:
        continue
        
    year_data = seasonal[seasonal['year'] == year].head(10)
    
    # Create input (previous years)
    prev_years = seasonal[(seasonal['year'] >= year-3) & (seasonal['year'] < year)]
    
    if len(prev_years) == 0:
        continue
    
    input_text = f"Predict the {year} F1 Constructor Championship based on recent history:\n\n"
    
    for prev_year in sorted(prev_years['year'].unique()):
        py_data = prev_years[prev_years['year'] == prev_year]
        input_text += f"Season {prev_year}:\n"
        for _, row in py_data.iterrows():
            input_text += f"- {row['name']}: {row['points']} pts, P{int(row['position'])}\n"
        input_text += "\n"
    
    # Create output (actual results)
    output_text = f"{year} Constructor Championship:\n"
    for i, (_, row) in enumerate(year_data.iterrows(), 1):
        output_text += f"{i}. {row['name']}: {int(row['points'])} points\n"
    
    training_data.append({
        "instruction": input_text,
        "response": output_text
    })

print(f"‚úÖ Created {len(training_data)} training examples!")

--------------------------------------------------------------------------------
# CELL 4: Prepare Dataset for Training (run after Cell 3)
--------------------------------------------------------------------------------

from datasets import Dataset

def format_prompt(example):
    return {
        "text": f"""### Instruction:
{example['instruction']}

### Response:
{example['response']}"""
    }

dataset = Dataset.from_list(training_data)
dataset = dataset.map(format_prompt)

print("‚úÖ Dataset ready!")
print(dataset)

--------------------------------------------------------------------------------
# CELL 5: Load Model with QLoRA (run after Cell 4, takes 2-3 min)
--------------------------------------------------------------------------------

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training

model_name = "Qwen/Qwen2.5-3B"

# 4-bit quantization config
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
)

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"

# Load model
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True,
)

model = prepare_model_for_kbit_training(model)

# LoRA config
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
)

model = get_peft_model(model, lora_config)

print("‚úÖ Model loaded with QLoRA!")
model.print_trainable_parameters()

--------------------------------------------------------------------------------
# CELL 6: Train the Model (run after Cell 5, takes 30-45 min)
--------------------------------------------------------------------------------

from trl import SFTTrainer
from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="./f1-qwen-lora",
    num_train_epochs=3,
    per_device_train_batch_size=2,
    gradient_accumulation_steps=4,
    learning_rate=2e-4,
    logging_steps=10,
    save_strategy="epoch",
    fp16=True,
    optim="paged_adamw_8bit",
    warmup_ratio=0.03,
    report_to="none",
)

trainer = SFTTrainer(
    model=model,
    train_dataset=dataset,
    args=training_args,
    tokenizer=tokenizer,
    dataset_text_field="text",
    max_seq_length=1024,
)

print("üöÄ Starting training... This will take 30-45 minutes.")
print("‚òï Go grab a coffee!")

trainer.train()

print("‚úÖ Training complete!")

--------------------------------------------------------------------------------
# CELL 7: Save and Upload Adapter (run after Cell 6)
--------------------------------------------------------------------------------

# Save locally
model.save_pretrained("./f1-qwen-lora-final")
tokenizer.save_pretrained("./f1-qwen-lora-final")

# Upload to Hugging Face
from huggingface_hub import HfApi

api = HfApi()

# Create repo (change YOUR_USERNAME to your Hugging Face username)
repo_name = "f1-predictor-qwen-lora"

try:
    api.create_repo(repo_name, private=True)
except:
    pass  # Repo might already exist

# Upload
api.upload_folder(
    folder_path="./f1-qwen-lora-final",
    repo_id=f"{api.whoami()['name']}/{repo_name}",
)

print("‚úÖ Adapter uploaded to Hugging Face!")
print(f"üì• Download from: https://huggingface.co/{api.whoami()['name']}/{repo_name}")

--------------------------------------------------------------------------------
# CELL 8: Test Your Model (optional, run after Cell 7)
--------------------------------------------------------------------------------

test_prompt = """### Instruction:
Predict the 2024 F1 Constructor Championship based on recent history:

Season 2021:
- Mercedes: 613.5 pts, P1
- Red Bull: 585.5 pts, P2
- Ferrari: 323.5 pts, P3

Season 2022:
- Red Bull: 759 pts, P1
- Ferrari: 554 pts, P2
- Mercedes: 515 pts, P3

Season 2023:
- Red Bull: 860 pts, P1
- Mercedes: 409 pts, P2
- Ferrari: 406 pts, P3

### Response:
"""

inputs = tokenizer(test_prompt, return_tensors="pt").to("cuda")
outputs = model.generate(**inputs, max_new_tokens=200, temperature=0.7)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))

--------------------------------------------------------------------------------
END OF CODE
--------------------------------------------------------------------------------


================================================================================
PART 5: RUN THE NOTEBOOK (45-60 minutes)
================================================================================

STEP 1: Run Each Cell in Order
------------------------------
Click each cell and press Shift+Enter (or click the Play button)
Wait for each cell to finish before running the next one.

Cell 1: ~2-3 minutes (installing packages)
Cell 2: ~10 seconds (login)
Cell 3: ~30 seconds (data prep)
Cell 4: ~10 seconds (dataset)
Cell 5: ~2-3 minutes (loading model)
Cell 6: ~30-45 minutes (TRAINING - go get coffee!)
Cell 7: ~2-3 minutes (upload)
Cell 8: ~30 seconds (test)

STEP 2: Note Your Model URL
---------------------------
After Cell 7, you'll see a URL like:
https://huggingface.co/YOUR_USERNAME/f1-predictor-qwen-lora

SAVE THIS URL! You need it for the next part.


================================================================================
PART 6: USE ON YOUR MAC (10 minutes)
================================================================================

STEP 1: Download Adapter Files
------------------------------
1. Go to your Hugging Face model URL
2. Click "Files and versions"
3. Download ALL files to a folder called "f1-qwen-lora" on your Mac

STEP 2: The adapter works with Ollama or transformers
------------------------------------------------------
Your LoRA adapter can be used with:
- Ollama (create custom model with adapter)
- Transformers library (load base + adapter in Python)

STEP 3: Update Your Project
---------------------------
I'll update your project files to use the fine-tuned model.
(Tell me when you've downloaded the adapter files)


================================================================================
TROUBLESHOOTING
================================================================================

ERROR: "GPU not available"
--------------------------
‚Üí You didn't enable GPU. Go to notebook settings and select GPU P100.
‚Üí Or you didn't phone verify. Do Part 1, Step 2.

ERROR: "Access denied" or "gated repo"  
---------------------------------------
‚Üí Your HF_TOKEN is wrong. Regenerate it in Hugging Face settings.

ERROR: "Out of memory"
----------------------
‚Üí Reduce batch size in Cell 6: per_device_train_batch_size=1

ERROR: "Secret not found"
-------------------------
‚Üí You didn't add HF_TOKEN secret. Do Part 3, Step 4.

NOTEBOOK KEEPS DISCONNECTING
----------------------------
‚Üí Normal on free tier. Just re-run from the cell that failed.
‚Üí Training progress is saved at each epoch.


================================================================================
QUICK REFERENCE
================================================================================

Kaggle: https://www.kaggle.com
Hugging Face: https://huggingface.co
Qwen Model: https://huggingface.co/Qwen/Qwen2.5-3B

Total Time: ~1 hour
Cost: FREE

================================================================================
